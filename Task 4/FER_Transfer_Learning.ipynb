{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7310b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bad0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234afad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e22c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(\"train\", transform=train_transform)\n",
    "test_dataset = ImageFolder(\"test\", transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Classes:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cadc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c10995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18(num_classes, partial=True):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    if partial:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767b3688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with partial fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.6268\n",
      "Epoch 2: Loss = 1.5382\n",
      "Epoch 3: Loss = 1.5147\n",
      "Epoch 4: Loss = 1.5133\n",
      "Epoch 5: Loss = 1.5008\n",
      "\n",
      "Training with full fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.2890\n",
      "Epoch 2: Loss = 1.1076\n",
      "Epoch 3: Loss = 1.0368\n",
      "Epoch 4: Loss = 0.9753\n",
      "Epoch 5: Loss = 0.9369\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for mode in [\"partial\", \"full\"]:\n",
    "    print(f\"\\nTraining with {mode} fine-tuning\")\n",
    "\n",
    "    model = get_resnet18(num_classes, partial=(mode==\"partial\"))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "    y_true, y_pred = evaluate(model, test_loader)\n",
    "\n",
    "    results[mode] = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"cm\": confusion_matrix(y_true, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4894535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARTIAL FINE-TUNING\n",
      "accuracy: 0.4287\n",
      "precision: 0.4235\n",
      "recall: 0.4287\n",
      "f1: 0.4078\n",
      "\n",
      "FULL FINE-TUNING\n",
      "accuracy: 0.6364\n",
      "precision: 0.6500\n",
      "recall: 0.6364\n",
      "f1: 0.6384\n"
     ]
    }
   ],
   "source": [
    "for mode, metrics in results.items():\n",
    "    print(f\"\\n{mode.upper()} FINE-TUNING\")\n",
    "    for k,v in metrics.items():\n",
    "        if k != \"cm\":\n",
    "            print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1837a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARTIAL FINE-TUNING\n",
      "accuracy: 0.4287\n",
      "precision: 0.4235\n",
      "recall: 0.4287\n",
      "f1: 0.4078\n",
      "\n",
      "FULL FINE-TUNING\n",
      "accuracy: 0.6364\n",
      "precision: 0.6500\n",
      "recall: 0.6364\n",
      "f1: 0.6384\n"
     ]
    }
   ],
   "source": [
    "for mode, metrics in results.items():\n",
    "    print(f\"\\n{mode.upper()} FINE-TUNING\")\n",
    "    for k,v in metrics.items():\n",
    "        if k != \"cm\":\n",
    "            print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee73901",
   "metadata": {},
   "source": [
    "### Model Choice\n",
    "ResNet-18 was used for the Facial Expression Recognition task because facial expressions depend on small and subtle features such as eye shape, mouth movement, and facial muscle changes. The residual connections in ResNet-18 help the model learn these features effectively without training issues like degradation. Its moderate depth is suitable for facial datasets, as it is deep enough to capture important patterns while reducing the risk of overfitting. Using a pre-trained ResNet-18 also helps the model start with useful visual features learned from large image datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
