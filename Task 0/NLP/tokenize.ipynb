{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3958a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=30000, min_frequency=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def train(self, texts):\n",
    "        self.tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        trainer = WordPieceTrainer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            min_frequency=self.min_frequency,\n",
    "            special_tokens=[\"[UNK]\", \"[PAD]\"]\n",
    "        )\n",
    "\n",
    "        self.tokenizer.train_from_iterator(texts, trainer)\n",
    "        self.tokenizer.decoder = WordPieceDecoder(prefix=\"##\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text).ids\n",
    "\n",
    "    def encode_tokens(self, text):\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a377114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e6cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_pairs(encoded_text, window_size):\n",
    "    pairs = []\n",
    "    n = len(encoded_text)\n",
    "    for i in range(n):\n",
    "        target = encoded_text[i]\n",
    "        context = []\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i and 0 <= j < n:\n",
    "                context.append(encoded_text[j])\n",
    "        if context:\n",
    "            pairs.append((context, target))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5268332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.pairs[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7bded89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "\n",
    "    max_len = max(len(c) for c in contexts)\n",
    "    padded_contexts = []\n",
    "\n",
    "    pad_id = 1\n",
    "    \n",
    "    for c in contexts:\n",
    "        pad = torch.full(max_len - len(c), pad_id, dtype=torch.long)\n",
    "        padded_contexts.append(torch.cat([c, pad]))\n",
    "\n",
    "    context_tensor = torch.stack(padded_contexts)   # [B, C]\n",
    "    target_tensor = torch.stack(targets)            # [B]\n",
    "\n",
    "    return context_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26049335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neg_dist(encoded_corpus, vocab_size):\n",
    "    counts = torch.zeros(vocab_size)\n",
    "    for t in encoded_corpus:\n",
    "        counts[t] += 1\n",
    "    dist = counts ** 0.75\n",
    "    dist /= dist.sum()\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23cf81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\tdef __init__(self, vocab_size, emb_dim): \n",
    "\t\tsuper().__init__() \n",
    "\t\tself.in_embed = nn.Embedding(vocab_size, emb_dim) \n",
    "\t\tself.out_embed = nn.Embedding(vocab_size, emb_dim) \n",
    "\tdef forward(self, context_ids, target_ids, neg_ids): \n",
    "\t\t# context_ids: [B,C] \n",
    "\t\t# target_ids: [B]\n",
    "\t\t# neg_ids: [B,K] \n",
    "\t\tcontext_vec = self.in_embed(context_ids) # [B,C,D] \n",
    "\t\tcontext_mean = context_vec.mean(dim=1) # [B,D] \n",
    "\t\t\n",
    "\t\ttarget_vec = self.out_embed(target_ids) # [B,D] \n",
    "\t\tneg_vec = self.out_embed(neg_ids) # [B,K,D] \n",
    "\t\t\n",
    "\t\tpos_score = torch.sum(context_mean * target_vec, dim=1) # [B] \n",
    "\t\tpos_loss = torch.log(torch.sigmoid(pos_score) + 1e-9) \n",
    "\t\t\n",
    "\t\tneg_score = torch.bmm(neg_vec, context_mean.unsqueeze(2)).squeeze(2) # [B,K] \n",
    "\t\tneg_loss = torch.log(torch.sigmoid(-neg_score) + 1e-9).sum(dim=1) \n",
    "\t\t\n",
    "\t\tloss = -(pos_loss + neg_loss).mean() \n",
    "\t\treturn loss\n",
    "\t    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e146d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def train_cbow(encoded_corpus, vocab_size, window_size, emb_dim=384,\n",
    "               batch_size=128, neg_k=10, epochs=5, lr=0.002):\n",
    "\n",
    "    pairs = generate_cbow_pairs(encoded_corpus, window_size)\n",
    "    dataset = CBOWDataset(pairs)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    neg_dist = build_neg_dist(encoded_corpus, vocab_size)\n",
    "\n",
    "    def sample_neg(batch_size, k):\n",
    "        return torch.multinomial(neg_dist, batch_size * k, replacement=True).view(batch_size, k)\n",
    "\n",
    "    model = CBOW(vocab_size, emb_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for context_ids, target_ids in pbar:\n",
    "            B = context_ids.size(0)\n",
    "            neg_ids = sample_neg(B, neg_k)\n",
    "\n",
    "            loss = model(context_ids, target_ids, neg_ids)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss {total_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b858caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AllCombined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "tokenizer = WordPieceTokenizer(vocab_size=30000)\n",
    "tokenizer.train(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9637e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD id: 1\n"
     ]
    }
   ],
   "source": [
    "pad_id = tokenizer.tokenizer.token_to_id(\"[PAD]\")\n",
    "print(\"PAD id:\", pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d596545e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 5 (3034930092.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mencoded_corpus.extend(tokenizer.encode(t))\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 5\n"
     ]
    }
   ],
   "source": [
    "encoded_corpus = []\n",
    "i=0\n",
    "for t in texts:\n",
    "    i+=1\n",
    "    if i%10000 == 0:\n",
    "    encoded_corpus.extend(tokenizer.encode(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f534d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_cbow(encoded_corpus, vocab_size=30000, window_size=5, emb_dim=384,\n",
    "               batch_size=512, neg_k=10, epochs=3, lr=0.003)\n",
    "embeddings = model.in_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08504e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_vector(text, tokenizer, embeddings):\n",
    "    ids = tokenizer.encode(text)\n",
    "    \n",
    "    if len(ids) == 0:\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    vecs = embeddings[ids]          # [T,384]\n",
    "    doc_vec = vecs.mean(dim=0)      # [384]\n",
    "    return doc_vec.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371441ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_corpus(texts, tokenizer, embeddings):\n",
    "    X = []\n",
    "    for t in texts:\n",
    "        X.append(build_doc_vector(t, tokenizer, embeddings))\n",
    "    return np.vstack(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94311555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lines = open(\"Sentences_50Agree.txt\").read().splitlines()\n",
    "texts = []\n",
    "labels = []\n",
    "label_map = {\n",
    "    \".@negative\": 0,\n",
    "    \".@neutral\": 1,\n",
    "    \".@positive\": 2\n",
    "}\n",
    "for line in lines:\n",
    "    parts = line.rsplit(\" \", 1)   # split last token\n",
    "    texts.append(parts[0])\n",
    "    labels.append(label_map[parts[1]])\n",
    "X_train = vectorize_corpus(texts, tokenizer, embeddings)\n",
    "y_train = labels\n",
    "lines = open(\"Sentences_AllAgree.txt\").read().splitlines()\n",
    "texts = []\n",
    "labels = []\n",
    "for line in lines:\n",
    "    parts = line.rsplit(\" \", 1)   # split last token\n",
    "    texts.append(parts[0])\n",
    "    labels.append(label_map[parts[1]])\n",
    "X_test  = vectorize_corpus(texts, tokenizer, embeddings)\n",
    "y_test = labels\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000, solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, pred, average=\"macro\")\n",
    "print(\"CBOW F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5384b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_predict(texts):\n",
    "    preds = []\n",
    "    for t in texts:\n",
    "        score = sia.polarity_scores(t)[\"compound\"]\n",
    "        \n",
    "        if score >= 0.05:\n",
    "            preds.append(2)\n",
    "        elif score <= -0.05:\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(1)\n",
    "    return preds\n",
    "\n",
    "vader_pred = vader_predict(texts)\n",
    "vader_f1 = f1_score(y_test, vader_pred, average=\"macro\")\n",
    "\n",
    "print(\"VADER F1:\", vader_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
