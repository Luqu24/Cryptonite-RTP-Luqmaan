{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14847107,"sourceType":"datasetVersion","datasetId":9496180}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3958a092","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.trainers import WordPieceTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.decoders import WordPiece as WordPieceDecoder\nfrom tqdm.auto import tqdm\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:28:54.604517Z","iopub.execute_input":"2026-02-15T16:28:54.604848Z","iopub.status.idle":"2026-02-15T16:28:58.494247Z","shell.execute_reply.started":"2026-02-15T16:28:54.604811Z","shell.execute_reply":"2026-02-15T16:28:58.493307Z"}},"outputs":[],"execution_count":1},{"id":"eca28fbb-e0a7-46e0-af91-e90ac199e082","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:28:58.498716Z","iopub.execute_input":"2026-02-15T16:28:58.499298Z","iopub.status.idle":"2026-02-15T16:28:58.799041Z","shell.execute_reply.started":"2026-02-15T16:28:58.499260Z","shell.execute_reply":"2026-02-15T16:28:58.798170Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nNumber of GPUs: 2\n","output_type":"stream"}],"execution_count":2},{"id":"5fb6a061","cell_type":"code","source":"class WordPieceTokenizer:\n    def __init__(self, vocab_size=20000, min_frequency=2):\n        self.vocab_size = vocab_size\n        self.min_frequency = min_frequency\n        self.tokenizer = None\n    \n    def train(self, texts):\n        self.tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n        self.tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordPieceTrainer(\n            vocab_size=self.vocab_size,\n            min_frequency=self.min_frequency,\n            special_tokens=[\"[UNK]\", \"[PAD]\"]\n        )\n        self.tokenizer.train_from_iterator(texts, trainer)\n        self.tokenizer.decoder = WordPieceDecoder(prefix=\"##\")\n    \n    def encode(self, text):\n        return self.tokenizer.encode(text).ids\n    \n    def encode_tokens(self, text):\n        return self.tokenizer.encode(text).tokens\n    \n    def decode(self, ids):\n        return self.tokenizer.decode(ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:00.539020Z","iopub.execute_input":"2026-02-15T16:29:00.539339Z","iopub.status.idle":"2026-02-15T16:29:00.546409Z","shell.execute_reply.started":"2026-02-15T16:29:00.539312Z","shell.execute_reply":"2026-02-15T16:29:00.544939Z"}},"outputs":[],"execution_count":3},{"id":"f0e6cf2b","cell_type":"code","source":"class FastCBOWDataset(Dataset):\n    def __init__(self, encoded_corpus, window_size):\n        self.data = encoded_corpus\n        self.window_size = window_size\n    \n    def __len__(self):\n        return len(self.data) - 2 * self.window_size\n    \n    def __getitem__(self, idx):\n        actual_idx = idx + self.window_size\n        \n        # Convert to tensor here instead of storing as tensor\n        target = torch.tensor(self.data[actual_idx], dtype=torch.long)\n        \n        context_left = self.data[actual_idx - self.window_size : actual_idx]\n        context_right = self.data[actual_idx + 1 : actual_idx + self.window_size + 1]\n        \n        # Convert slices to tensors\n        context = torch.tensor(context_left + context_right, dtype=torch.long)\n        \n        return context, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:02.443449Z","iopub.execute_input":"2026-02-15T16:29:02.444292Z","iopub.status.idle":"2026-02-15T16:29:02.449978Z","shell.execute_reply.started":"2026-02-15T16:29:02.444262Z","shell.execute_reply":"2026-02-15T16:29:02.449144Z"}},"outputs":[],"execution_count":4},{"id":"e152cfb9-4012-4d1f-9798-9ce4d269b7fe","cell_type":"code","source":"class CBOW(nn.Module):\n    def __init__(self, vocab_size, emb_dim): \n        super().__init__() \n        self.in_embed = nn.Embedding(vocab_size, emb_dim) \n        self.out_embed = nn.Embedding(vocab_size, emb_dim) \n    \n    def forward(self, context_ids, target_ids, neg_ids): \n        context_vec = self.in_embed(context_ids)\n        context_mean = context_vec.mean(dim=1)\n        \n        target_vec = self.out_embed(target_ids)\n        neg_vec = self.out_embed(neg_ids)\n        \n        pos_score = torch.sum(context_mean * target_vec, dim=1)\n        pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-9)\n        \n        neg_score = torch.bmm(neg_vec, context_mean.unsqueeze(2)).squeeze(2)\n        neg_loss = torch.log(torch.sigmoid(-neg_score) + 1e-9).sum(dim=1)\n        \n        loss = -(pos_loss + neg_loss).mean()\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:04.385927Z","iopub.execute_input":"2026-02-15T16:29:04.386295Z","iopub.status.idle":"2026-02-15T16:29:04.393039Z","shell.execute_reply.started":"2026-02-15T16:29:04.386264Z","shell.execute_reply":"2026-02-15T16:29:04.392252Z"}},"outputs":[],"execution_count":5},{"id":"f7bded89","cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Optimized collate function\"\"\"\n    contexts, targets = zip(*batch)\n    \n    # Stack contexts directly (they're all same size for CBOW)\n    context_tensor = torch.stack(contexts)\n    target_tensor = torch.stack(targets)\n    \n    return context_tensor, target_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:06.069858Z","iopub.execute_input":"2026-02-15T16:29:06.070466Z","iopub.status.idle":"2026-02-15T16:29:06.075169Z","shell.execute_reply.started":"2026-02-15T16:29:06.070436Z","shell.execute_reply":"2026-02-15T16:29:06.074406Z"}},"outputs":[],"execution_count":6},{"id":"26049335","cell_type":"code","source":"def build_neg_dist(encoded_corpus, vocab_size):\n    \"\"\"Build negative sampling distribution\"\"\"\n    counts = torch.zeros(vocab_size)\n    for t in encoded_corpus:\n        counts[t] += 1\n    dist = counts ** 0.75\n    dist /= dist.sum()\n    return dist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:06.838743Z","iopub.execute_input":"2026-02-15T16:29:06.839364Z","iopub.status.idle":"2026-02-15T16:29:06.843955Z","shell.execute_reply.started":"2026-02-15T16:29:06.839334Z","shell.execute_reply":"2026-02-15T16:29:06.843118Z"}},"outputs":[],"execution_count":7},{"id":"23cf81ec","cell_type":"code","source":"def train_cbow(encoded_corpus, vocab_size, window_size, emb_dim=384,\n               batch_size=512, neg_k=10, epochs=5, lr=0.003):\n    \"\"\"\n    Optimized training with multi-GPU and mixed precision support\n    \"\"\"\n    print(\"Creating dataset...\")\n    dataset = FastCBOWDataset(encoded_corpus, window_size)\n    \n    print(\"Setting up DataLoader...\")\n    # Increased num_workers and batch size for better GPU utilization\n    loader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        collate_fn=collate_fn, \n        pin_memory=True, \n        num_workers=2,  # Increased workers\n        # persistent_workers=True  # Keep workers alive between epochs\n    )\n    \n    # Build negative sampling distribution on GPU\n    neg_dist = build_neg_dist(encoded_corpus, vocab_size).to(device)\n    \n    def sample_neg(batch_size, k):\n        \"\"\"Sample negative examples on GPU\"\"\"\n        return torch.multinomial(neg_dist, batch_size * k, replacement=True).view(batch_size, k)\n    \n    # Create model\n    model = CBOW(vocab_size, emb_dim)\n    \n    # Multi-GPU support\n    # if torch.cuda.device_count() > 1:\n    #     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    #     model = nn.DataParallel(model)\n    \n    model = model.to(device)\n    \n    # Optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    \n    # Mixed precision training\n    scaler = GradScaler()\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for context_ids, target_ids in pbar:\n            B = context_ids.size(0)\n            \n            # Sample negatives on GPU\n            neg_ids = sample_neg(B, neg_k)\n            \n            # Move to device\n            context_ids = context_ids.to(device, non_blocking=True)\n            target_ids = target_ids.to(device, non_blocking=True)\n            neg_ids = neg_ids.to(device, non_blocking=True)\n            \n            # Mixed precision forward pass\n            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n            \n            with autocast():\n                loss = model(context_ids, target_ids, neg_ids)\n            \n            # Backward pass with gradient scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            total_loss += loss.item()\n            pbar.set_postfix(loss=loss.item())\n\n            # if pbar.n % 100 == 0:\n                # torch.cuda.empty_cache()\n        \n        scheduler.step()\n        avg_loss = total_loss / len(loader)\n        print(f\"Epoch {epoch+1}: Avg Loss {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    # Return the base model if using DataParallel\n    if isinstance(model, nn.DataParallel):\n        return model.module\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:08.747211Z","iopub.execute_input":"2026-02-15T16:29:08.747502Z","iopub.status.idle":"2026-02-15T16:29:08.758205Z","shell.execute_reply.started":"2026-02-15T16:29:08.747478Z","shell.execute_reply":"2026-02-15T16:29:08.757312Z"}},"outputs":[],"execution_count":8},{"id":"53e146d8","cell_type":"code","source":"def vectorize_corpus_fast(texts, tokenizer, model, batch_size=512):\n    \"\"\"\n    Optimized vectorization with GPU batching\n    \"\"\"\n    model.eval()\n    \n    # Get base model if wrapped in DataParallel\n    if isinstance(model, nn.DataParallel):\n        embeddings = model.module.in_embed.weight.detach()\n    else:\n        embeddings = model.in_embed.weight.detach()\n    \n    all_vecs = []\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Vectorizing\"):\n            batch_texts = texts[i : i + batch_size]\n            encodings = tokenizer.tokenizer.encode_batch(batch_texts)\n            \n            for enc in encodings:\n                if len(enc.ids) == 0:\n                    all_vecs.append(np.zeros(embeddings.shape[1]))\n                else:\n                    ids = torch.tensor(enc.ids, device=embeddings.device)\n                    doc_vec = embeddings[ids].mean(dim=0).cpu().numpy()\n                    all_vecs.append(doc_vec)\n    \n    return np.vstack(all_vecs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:11.550452Z","iopub.execute_input":"2026-02-15T16:29:11.551306Z","iopub.status.idle":"2026-02-15T16:29:11.557741Z","shell.execute_reply.started":"2026-02-15T16:29:11.551277Z","shell.execute_reply":"2026-02-15T16:29:11.556896Z"}},"outputs":[],"execution_count":9},{"id":"bb157d84-45e9-4bfd-86ab-91c1450a6d23","cell_type":"code","source":"# Main training code\nprint(\"Loading data...\")\nwith open(\"/kaggle/input/datasets/luqu24who/datasettext/AllCombined.txt\", \"r\", encoding=\"utf-8\") as f:\n    texts = [line.strip() for line in f if line.strip()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:15.114761Z","iopub.execute_input":"2026-02-15T16:29:15.115072Z","iopub.status.idle":"2026-02-15T16:29:20.356370Z","shell.execute_reply.started":"2026-02-15T16:29:15.115046Z","shell.execute_reply":"2026-02-15T16:29:20.355395Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n","output_type":"stream"}],"execution_count":10},{"id":"5817bc74-52ef-45be-84dc-e2dc3cf8d002","cell_type":"code","source":"print(\"Training tokenizer...\")\ntokenizer = WordPieceTokenizer(vocab_size=20000)\ntokenizer.train(texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:20.357956Z","iopub.execute_input":"2026-02-15T16:29:20.358230Z","iopub.status.idle":"2026-02-15T16:29:40.935366Z","shell.execute_reply.started":"2026-02-15T16:29:20.358206Z","shell.execute_reply":"2026-02-15T16:29:40.934383Z"}},"outputs":[{"name":"stdout","text":"Training tokenizer...\n\n\n\n","output_type":"stream"}],"execution_count":11},{"id":"a8275085-d469-4ba1-8feb-9448242123b7","cell_type":"code","source":"pad_id = tokenizer.tokenizer.token_to_id(\"[PAD]\")\nprint(\"PAD id:\", pad_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:40.936854Z","iopub.execute_input":"2026-02-15T16:29:40.937206Z","iopub.status.idle":"2026-02-15T16:29:40.942224Z","shell.execute_reply.started":"2026-02-15T16:29:40.937179Z","shell.execute_reply":"2026-02-15T16:29:40.941524Z"}},"outputs":[{"name":"stdout","text":"PAD id: 1\n","output_type":"stream"}],"execution_count":12},{"id":"2ebb72d9-7800-4aab-bd50-47dcbb0c2fbb","cell_type":"code","source":"\nprint(\"Encoding corpus...\")\nencodings = tokenizer.tokenizer.encode_batch(texts)\nencoded_corpus = []\nfor enc in encodings:\n    encoded_corpus.extend(enc.ids)\n\nprint(f\"Corpus size: {len(encoded_corpus):,} tokens\")\nprint(f\"Memory: {len(encoded_corpus) * 8 / 1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:29:40.943247Z","iopub.execute_input":"2026-02-15T16:29:40.943778Z","iopub.status.idle":"2026-02-15T16:30:10.838410Z","shell.execute_reply.started":"2026-02-15T16:29:40.943744Z","shell.execute_reply":"2026-02-15T16:30:10.837528Z"}},"outputs":[{"name":"stdout","text":"Encoding corpus...\nCorpus size: 44,191,554 tokens\nMemory: 0.33 GB\n","output_type":"stream"}],"execution_count":13},{"id":"d1233d0e-68a7-4bd8-8e68-ee7c81cc5545","cell_type":"code","source":"# Train with optimized settings\nprint(\"Training CBOW model...\")\nmodel = train_cbow(\n    encoded_corpus, \n    vocab_size=20000, \n    window_size=5, \n    emb_dim=384,\n    batch_size=512,  # Increased for multi-GPU\n    neg_k=10, \n    epochs=2, \n    lr=0.003\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:49:18.181311Z","iopub.execute_input":"2026-02-15T16:49:18.182130Z","iopub.status.idle":"2026-02-15T17:31:33.713495Z","shell.execute_reply.started":"2026-02-15T16:49:18.182092Z","shell.execute_reply":"2026-02-15T17:31:33.712524Z"}},"outputs":[{"name":"stdout","text":"Training CBOW model...\nCreating dataset...\nSetting up DataLoader...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/105458180.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/2:   0%|          | 0/86312 [00:05<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54a8497b34f4f8786574186db6a0993"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/tmp/ipykernel_55/105458180.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Avg Loss inf, LR: 0.001500\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/2:   0%|          | 0/86312 [00:05<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384de388cfbe4624a86db853538b3380"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nException ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7962bf166e80><function _MultiProcessingDataLoaderIter.__del__ at 0x7962bf166e80>\n\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n      File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\nself._shutdown_workers()    \n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\nself._shutdown_workers()\n      File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\nif w.is_alive():    \nif w.is_alive():\n              ^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    \nassert self._parent_pid == os.getpid(), 'can only test a child process' \n                  ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^\n: AssertionErrorcan only test a child process: \ncan only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Avg Loss inf, LR: 0.000000\n","output_type":"stream"}],"execution_count":15},{"id":"648989ad-2587-4450-97a4-5d876ad8712c","cell_type":"code","source":"\ntorch.save(model.state_dict(), '/kaggle/working/cbow_model.pth')\ntokenizer.tokenizer.save('/kaggle/working/tokenizer.json')\n\nprint(\"Model and tokenizer saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T17:32:02.090809Z","iopub.execute_input":"2026-02-15T17:32:02.091725Z","iopub.status.idle":"2026-02-15T17:32:02.235392Z","shell.execute_reply.started":"2026-02-15T17:32:02.091645Z","shell.execute_reply":"2026-02-15T17:32:02.234737Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved!\n","output_type":"stream"}],"execution_count":16},{"id":"91c9cc5c-0137-41ee-b494-c5ab2481c6a8","cell_type":"code","source":"model = CBOW(vocab_size=20000, emb_dim=384)\nmodel.load_state_dict(torch.load('/kaggle/working/cbow_model.pth'))\nmodel.to(device)\nmodel.eval()\n\n# Load tokenizer\nfrom tokenizers import Tokenizer\nloaded_tokenizer = Tokenizer.from_file('/kaggle/working/tokenizer.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cedb3cb5-b35c-42a6-b0af-1177be7ff774","cell_type":"code","source":"if isinstance(model, nn.DataParallel):\n    embeddings = model.module.in_embed.weight\nelse:\n    embeddings = model.in_embed.weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T17:32:15.609793Z","iopub.execute_input":"2026-02-15T17:32:15.610092Z","iopub.status.idle":"2026-02-15T17:32:15.614484Z","shell.execute_reply.started":"2026-02-15T17:32:15.610068Z","shell.execute_reply":"2026-02-15T17:32:15.613645Z"}},"outputs":[],"execution_count":17},{"id":"b858caa0","cell_type":"code","source":"# Sentiment classification\nprint(\"\\nLoading sentiment data...\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nlines = open(\"/kaggle/input/datasets/luqu24who/datasettext/Sentences_50Agree.txt\", errors=\"ignore\").read().splitlines()\ntexts_train = []\nlabels_train = []\nlabel_map = {\n    \"negative\": 0,\n    \"neutral\": 1,\n    \"positive\": 2\n}\n\nfor line in lines:\n    parts = line.rsplit(\"@\", 1)\n    texts_train.append(parts[0])\n    labels_train.append(label_map[parts[1]])\n\nprint(\"Vectorizing training data...\")\nX_train = vectorize_corpus_fast(texts_train, tokenizer, model, batch_size=512)\ny_train = labels_train\n\nlines = open(\"/kaggle/input/datasets/luqu24who/datasettext/Sentences_AllAgree.txt\", errors=\"ignore\").read().splitlines()\ntexts_test = []\nlabels_test = []\n\nfor line in lines:\n    parts = line.rsplit(\"@\", 1)\n    texts_test.append(parts[0])\n    labels_test.append(label_map[parts[1]])\n\nprint(\"Vectorizing test data...\")\nX_test = vectorize_corpus_fast(texts_test, tokenizer, model, batch_size=512)\ny_test = labels_test\n\nprint(\"Training classifier...\")\nclf = LogisticRegression(max_iter=2000, solver=\"liblinear\")\nclf.fit(X_train, y_train)\n\npred = clf.predict(X_test)\nf1 = f1_score(y_test, pred, average=\"macro\")\nprint(f\"\\nCBOW F1: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T17:36:25.509328Z","iopub.execute_input":"2026-02-15T17:36:25.510033Z","iopub.status.idle":"2026-02-15T17:36:27.793235Z","shell.execute_reply.started":"2026-02-15T17:36:25.510001Z","shell.execute_reply":"2026-02-15T17:36:27.792462Z"}},"outputs":[{"name":"stdout","text":"\nLoading sentiment data...\nVectorizing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Vectorizing:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843b0c54979943719d4c2f43e257b15e"}},"metadata":{}},{"name":"stdout","text":"Vectorizing test data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Vectorizing:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd3adb52cb0946989f91de9b67ae358f"}},"metadata":{}},{"name":"stdout","text":"Training classifier...\n\nCBOW F1: 0.7819\n","output_type":"stream"}],"execution_count":25},{"id":"c9637e43","cell_type":"code","source":"# VADER comparison\nprint(\"\\nComparing with VADER...\")\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\ndef vader_predict(texts):\n    preds = []\n    for t in texts:\n        score = sia.polarity_scores(t)[\"compound\"]\n        if score >= 0.05:\n            preds.append(2)\n        elif score <= -0.05:\n            preds.append(0)\n        else:\n            preds.append(1)\n    return preds\n\nvader_pred = vader_predict(texts_test)\nvader_f1 = f1_score(y_test, vader_pred, average=\"macro\")\nprint(f\"VADER F1: {vader_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T17:36:44.926400Z","iopub.execute_input":"2026-02-15T17:36:44.926769Z","iopub.status.idle":"2026-02-15T17:36:45.799151Z","shell.execute_reply.started":"2026-02-15T17:36:44.926701Z","shell.execute_reply":"2026-02-15T17:36:45.798397Z"}},"outputs":[{"name":"stdout","text":"\nComparing with VADER...\nVADER F1: 0.4867\n","output_type":"stream"}],"execution_count":26}]}