{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T15:24:26.283036Z",
     "iopub.status.busy": "2026-01-28T15:24:26.282513Z",
     "iopub.status.idle": "2026-01-28T15:24:26.287961Z",
     "shell.execute_reply": "2026-01-28T15:24:26.286773Z",
     "shell.execute_reply.started": "2026-01-28T15:24:26.283002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T15:27:28.260185Z",
     "iopub.status.busy": "2026-01-28T15:27:28.259792Z",
     "iopub.status.idle": "2026-01-28T15:27:28.273629Z",
     "shell.execute_reply": "2026-01-28T15:27:28.272757Z",
     "shell.execute_reply.started": "2026-01-28T15:27:28.260156Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in dataset: 3427466\n"
     ]
    }
   ],
   "source": [
    "with open(\"office_script_clean.txt\", \"r\", encoding=\"utf-8\") as f: #utf-8 tells python how to read bytes from file\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Total characters in dataset:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T15:27:44.434287Z",
     "iopub.status.busy": "2026-01-28T15:27:44.433894Z",
     "iopub.status.idle": "2026-01-28T15:27:44.869159Z",
     "shell.execute_reply": "2026-01-28T15:27:44.868033Z",
     "shell.execute_reply.started": "2026-01-28T15:27:44.434256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 72\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)} # dictionary mapping\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "text_encoded = np.array([char2idx[c] for c in text])\n",
    "# assigned each character a number between 0 to 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T15:29:35.455287Z",
     "iopub.status.busy": "2026-01-28T15:29:35.454861Z",
     "iopub.status.idle": "2026-01-28T15:29:35.888634Z",
     "shell.execute_reply": "2026-01-28T15:29:35.887694Z",
     "shell.execute_reply.started": "2026-01-28T15:29:35.455254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=100):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length # if like 100 characters in data set and sequence length 10 u can start sequence at positions 0 to 90 \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx: idx + self.seq_length] # not pre storing all sequnces coz wastes memory\n",
    "        y = self.data[idx + 1: idx + self.seq_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "seq_length = 100\n",
    "dataset = CharDataset(text_encoded, seq_length)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # gradients become correlated, overfitting risk if no shuffling\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True) # drop last = true if total samples not divisible by 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T15:30:28.179527Z",
     "iopub.status.busy": "2026-01-28T15:30:28.179221Z",
     "iopub.status.idle": "2026-01-28T15:30:28.186002Z",
     "shell.execute_reply": "2026-01-28T15:30:28.185109Z",
     "shell.execute_reply.started": "2026-01-28T15:30:28.179503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(x) # output contains hidden state at each time step just like weather forecasting one, hidden is final a and c of each sequence\n",
    "        else:\n",
    "            output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T15:31:00.302103Z",
     "iopub.status.busy": "2026-01-28T15:31:00.301569Z",
     "iopub.status.idle": "2026-01-28T15:52:16.956406Z",
     "shell.execute_reply": "2026-01-28T15:52:16.954261Z",
     "shell.execute_reply.started": "2026-01-28T15:31:00.302064Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 24098/24098 [14:24<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.0859 | Val Loss: 0.9934 | Val PPL: 2.7004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 24098/24098 [14:40<00:00, 27.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.9654 | Val Loss: 0.9491 | Val PPL: 2.5833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 24098/24098 [13:00<00:00, 30.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.9388 | Val Loss: 0.9345 | Val PPL: 2.5460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 24098/24098 [12:06<00:00, 33.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.9276 | Val Loss: 0.9277 | Val PPL: 2.5287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 24098/24098 [12:08<00:00, 33.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.9218 | Val Loss: 0.9225 | Val PPL: 2.5157\n"
     ]
    }
   ],
   "source": [
    "model = CharLSTM(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(loss)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"): #tqdm to show progress\n",
    "        x, y = x.to(device), y.to(device) # note that y was never embedded\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1)) # logits is batchsize, sequence length, vocab size  y is just batchsize, sequence length whose value is integer which is index of currect word in vocab\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val PPL: {calculate_perplexity(torch.tensor(avg_val_loss)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-28T15:52:16.957864Z",
     "iopub.status.idle": "2026-01-28T15:52:16.958202Z",
     "shell.execute_reply": "2026-01-28T15:52:16.958078Z",
     "shell.execute_reply.started": "2026-01-28T15:52:16.958060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_script(model, seed_text, temperature=1.0, num_chars=500):\n",
    "    model.eval()\n",
    "    generated = seed_text\n",
    "    input_seq = torch.tensor([char2idx[c] for c in seed_text], dtype=torch.long).unsqueeze(0).to(device) # inital shape was (seedtext's length, ) after unsqueeze (0, len)\n",
    "    hidden = None\n",
    "    \n",
    "    for _ in range(num_chars):\n",
    "        logits, hidden = model(input_seq, hidden)\n",
    "        # model.forward is called - embedds, lstm gives output (hidden final memory), fc gives logits\n",
    "        logits = logits[:, -1, :] / temperature # final output of sequences divided by temp, shape is (batch size, V) = (1, vocab size)\n",
    "        probs = torch.softmax(logits, dim=-1) \n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item() # random sampling weighted by probability\n",
    "        next_char = idx2char[next_idx] \n",
    "        generated += next_char\n",
    "        input_seq = torch.tensor([[next_idx]], dtype=torch.long).to(device) # embedding layer will do the embedding, only giving one character but hidden layer holds context\n",
    "    \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-28T15:52:16.959587Z",
     "iopub.status.idle": "2026-01-28T15:52:16.959983Z",
     "shell.execute_reply": "2026-01-28T15:52:16.959851Z",
     "shell.execute_reply.started": "2026-01-28T15:52:16.959830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Temp 0.3 ===\n",
      " Michael: Alright. Well, I guess I could be ready.\n",
      "Michael: What are you doing here?\n",
      "Andy: I have a song on a paper company in the world. And then you will not be a formal guy, and then I was thinking about some decisive on you. I was a family on Pam falls away from me. And I can make sure that you were the same time that I want to see the word I can be confidence and prepared to confinnie with her. I think I go to the store in the workplace.\n",
      "Michael: Well, I can't be so much subshing company for a while. I don't know when they are going to take a look at the top to the machine that they are not a completely disressing the office. I would love that. That is a good idea to go to the store and I said I was gonna be my father anymore. I want you to be more specific. Welcome to the store in the world. And I will see you tonight. I think it's a good sales call.\n",
      "Kevin: What is that?\n",
      "Angela: Yes. In a way to get a strong conseque. I saw the paper in the face of the day to make sure that this morning, w\n",
      "=== Temp 0.7 ===\n",
      " Michael: Oh, my God, thank you sir! \n",
      "Michael: Damnit has divorced into a bunch of steady, I'm calling the courthouse.\n",
      "Jim: Right.\n",
      "Girl in Scranton: It's right away from Foss.\n",
      "Michael:  What are you doing? I would say that I was in an arpanic way. I mean, who knows? No, that was pretty cool. We want to say that I am a fan, it's time for the sales stuff.\n",
      "Pam: Hey, what do you think you're talking about?\n",
      "Michael: Aaaaaaaaa!\n",
      "Michael: That was a waste. \n",
      "Dwight: And here is her recommendation for the star a really good point. I mean, when he's staying in the lowest character named Willight.\n",
      "Dwight: ...and I was fine. It's a male hour!  You need to control him to think the stuff with Erin's like thrilled by a boyfriend's being a little bit more important that we can finish the car to get the best Christmas party in a bit of a boss and a hero color to the restructure hair. Fact: You mean that could they believe it was a stupid house.\n",
      "Pam: What are you?\n",
      "Pam: Are you still fine, Kevin? Those aren't weird\n",
      "=== Temp 1.0 ===\n",
      " Michael: I am feelin' gonna help your printers from dapa for human, solid in that.\n",
      "Jim: We all have farreet for a ring for a rough quirky in an her cake, cupcake. I'm out.\n",
      "Dwight: So, as place like that internet senator's butly, for stuff lie: my trip is interesting friends.\n",
      "Michael: See you.\n",
      "Toby: Fine, proic.\n",
      "Pam: Oh my gosh, that's just...\n",
      "Michael: Next hour, Drive..... O... Oh, keep me great!\n",
      "Dwight: Oh yeah... Ryan!\n",
      "Pam: Michael, no.\n",
      "Andy: Well, he just wasn't a brain tonight... Oh, this is cleave. Real his baby.\n",
      "Michael: And we are, I'll just be more businessed. I believe I was on the web.\n",
      "Michael: I, I would love that, get ready?\n",
      "Phyllis: Four! No, no! No!\n",
      "Dwight:  Thank you videot!\n",
      "Dwight:  Outside injuryin. I'll be the one who is happening in the same rule, but I don't need to you move over here.\n",
      "Karen: Yeah, I guess. Um, please heavy, confermatic, scene: Dude, go ahead.\n",
      "Pam: Mornin'.\n",
      "Marie: Over a good idea about the scene.\n",
      "Michael: Listen Guardn's Ryan, do you have my animal sheep?\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "seed = \"Michael: \"\n",
    "sample_1 = generate_script(model, seed, temperature=0.3, num_chars=1000)\n",
    "sample_2 = generate_script(model, seed, temperature=0.7, num_chars=1000)\n",
    "sample_3 = generate_script(model, seed, temperature=1.0, num_chars=1000)\n",
    "\n",
    "print(\"=== Temp 0.3 ===\\n\", sample_1)\n",
    "print(\"=== Temp 0.7 ===\\n\", sample_2)\n",
    "print(\"=== Temp 1.0 ===\\n\", sample_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9360940,
     "sourceId": 14653376,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
